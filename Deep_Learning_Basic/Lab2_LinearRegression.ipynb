{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lab1_1.png\">\n",
    "\n",
    "### Step1. Building graph using TF operations\n",
    "<img src=\"Images/lab2_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# X and Y data\n",
    "x_train = [1, 2, 3] #학습 데이터\n",
    "y_train = [1, 2, 3] #정답 레이블\n",
    "\n",
    "# Try to find values for W and b to compute y_data = x_data * W + b\n",
    "# We know that W should be 1 and b should be 0\n",
    "# But let TensorFlow figure it out\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\") #random_normal()은 타입을 정의. 여기서는 값이 하나인 1차원 배열\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "#Variable 노드는 다른 언어의 Variable과 달리, 실행 시 텐서플로우가 변경시키는 값이라고 생각하면 이해하기 편하다.(학습하는 과정에서 변경시킨다.)\n",
    "#tf.placeholder()는 입력 데이터를 만들 때 주로 사용한다. (실제 훈련 예제를 제공하는 변수) - 초기값을 지정할 필요 없다. (모델 입력시 변경되지 않을 데이터)\n",
    "#tf.Variable()은 데이터의 상태를 저장할 때 주로 사용한다. (가중치나 편향 등의 학습 가능한 변수) - 초기값을 지정해야 한다. (학습 되는 데이터)\n",
    "#http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = x_train * W + b #여기서 hypothesis는 (1x3)이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/lab2_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train)) #reduce_mean은 평균을 계산한다. #square는 제곱.\n",
    "#hypothesis가 (1x3)이므로(각 예상치 값) hypothesis에서 정답 레이블(y_train)을 빼주면 각각의 요소에 맞춰 뺄셈을 한 후 제곱한다.\n",
    "\n",
    "# Minimize : Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) \n",
    "#tf.train는 모델을 훈련하는 데 도움이되는 일련의 클래스와 함수를 제공한다.\n",
    "#train.GradientDescentOptimizer로 Gradient Descent 구현\n",
    "train = optimizer.minimize(cost) #비용을 최소화시킨다. -> 최적화 W와 b의 최적의 값 학습\n",
    "#propagation과 backpropagation을 실행해서 최적화 시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2, 3. Run/update graph and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13.3212 [-0.4379099] [-0.5800125]\n",
      "20 0.121029 [ 0.84647644] [-0.01747727]\n",
      "40 0.00132425 [ 0.96955526] [ 0.03431277]\n",
      "60 0.000218085 [ 0.98201853] [ 0.0375534]\n",
      "80 0.00018914 [ 0.98391414] [ 0.03625064]\n",
      "100 0.0001717 [ 0.98477012] [ 0.03459098]\n",
      "120 0.00015594 [ 0.98549539] [ 0.03296952]\n",
      "140 0.000141626 [ 0.98617798] [ 0.03142046]\n",
      "160 0.000128627 [ 0.98682767] [ 0.02994384]\n",
      "180 0.000116823 [ 0.98744673] [ 0.02853659]\n",
      "200 0.0001061 [ 0.98803669] [ 0.02719548]\n",
      "220 9.63619e-05 [ 0.98859882] [ 0.02591741]\n",
      "240 8.75164e-05 [ 0.98913461] [ 0.02469942]\n",
      "260 7.94857e-05 [ 0.98964536] [ 0.02353867]\n",
      "280 7.21882e-05 [ 0.99013209] [ 0.02243238]\n",
      "300 6.55623e-05 [ 0.99059582] [ 0.02137808]\n",
      "320 5.95443e-05 [ 0.99103779] [ 0.02037333]\n",
      "340 5.40786e-05 [ 0.99145895] [ 0.01941584]\n",
      "360 4.91151e-05 [ 0.99186039] [ 0.01850336]\n",
      "380 4.46076e-05 [ 0.99224287] [ 0.01763377]\n",
      "400 4.05132e-05 [ 0.99260741] [ 0.01680505]\n",
      "420 3.67946e-05 [ 0.99295485] [ 0.01601528]\n",
      "440 3.34178e-05 [ 0.99328589] [ 0.01526263]\n",
      "460 3.03506e-05 [ 0.9936015] [ 0.01454534]\n",
      "480 2.75652e-05 [ 0.99390215] [ 0.01386175]\n",
      "500 2.50348e-05 [ 0.99418879] [ 0.01321032]\n",
      "520 2.27373e-05 [ 0.99446189] [ 0.01258948]\n",
      "540 2.06504e-05 [ 0.99472207] [ 0.01199783]\n",
      "560 1.87553e-05 [ 0.99497008] [ 0.01143403]\n",
      "580 1.70341e-05 [ 0.99520653] [ 0.01089669]\n",
      "600 1.54706e-05 [ 0.99543184] [ 0.0103846]\n",
      "620 1.40503e-05 [ 0.99564648] [ 0.00989656]\n",
      "640 1.27607e-05 [ 0.9958511] [ 0.00943145]\n",
      "660 1.15895e-05 [ 0.99604613] [ 0.00898818]\n",
      "680 1.05261e-05 [ 0.99623191] [ 0.00856576]\n",
      "700 9.55927e-06 [ 0.996409] [ 0.00816318]\n",
      "720 8.68218e-06 [ 0.99657768] [ 0.00777957]\n",
      "740 7.88509e-06 [ 0.99673855] [ 0.007414]\n",
      "760 7.16177e-06 [ 0.99689186] [ 0.00706559]\n",
      "780 6.50429e-06 [ 0.99703795] [ 0.00673353]\n",
      "800 5.9073e-06 [ 0.99717712] [ 0.00641707]\n",
      "820 5.36491e-06 [ 0.9973098] [ 0.00611549]\n",
      "840 4.87268e-06 [ 0.99743623] [ 0.00582809]\n",
      "860 4.42522e-06 [ 0.99755675] [ 0.0055542]\n",
      "880 4.01948e-06 [ 0.99767154] [ 0.00529317]\n",
      "900 3.65035e-06 [ 0.99778098] [ 0.0050444]\n",
      "920 3.31545e-06 [ 0.99788523] [ 0.00480734]\n",
      "940 3.01103e-06 [ 0.99798459] [ 0.00458142]\n",
      "960 2.73454e-06 [ 0.99807936] [ 0.00436612]\n",
      "980 2.48376e-06 [ 0.99816954] [ 0.00416096]\n",
      "1000 2.25602e-06 [ 0.99825561] [ 0.00396544]\n",
      "1020 2.04868e-06 [ 0.99833757] [ 0.00377908]\n",
      "1040 1.86062e-06 [ 0.99841571] [ 0.00360148]\n",
      "1060 1.68983e-06 [ 0.99849015] [ 0.00343221]\n",
      "1080 1.53494e-06 [ 0.99856108] [ 0.00327091]\n",
      "1100 1.39393e-06 [ 0.99862868] [ 0.00311722]\n",
      "1120 1.26614e-06 [ 0.99869317] [ 0.00297075]\n",
      "1140 1.14979e-06 [ 0.99875456] [ 0.00283113]\n",
      "1160 1.04433e-06 [ 0.99881309] [ 0.00269808]\n",
      "1180 9.48451e-07 [ 0.99886888] [ 0.0025713]\n",
      "1200 8.61529e-07 [ 0.99892205] [ 0.00245049]\n",
      "1220 7.82403e-07 [ 0.99897271] [ 0.00233534]\n",
      "1240 7.10549e-07 [ 0.99902099] [ 0.0022256]\n",
      "1260 6.45294e-07 [ 0.99906695] [ 0.00212102]\n",
      "1280 5.86172e-07 [ 0.99911076] [ 0.00202135]\n",
      "1300 5.32434e-07 [ 0.99915254] [ 0.00192638]\n",
      "1320 4.83565e-07 [ 0.99919242] [ 0.00183586]\n",
      "1340 4.39236e-07 [ 0.99923027] [ 0.00174961]\n",
      "1360 3.98849e-07 [ 0.99926651] [ 0.00166742]\n",
      "1380 3.62326e-07 [ 0.9993009] [ 0.00158909]\n",
      "1400 3.29067e-07 [ 0.99933374] [ 0.00151444]\n",
      "1420 2.98801e-07 [ 0.99936509] [ 0.0014433]\n",
      "1440 2.71401e-07 [ 0.99939489] [ 0.00137549]\n",
      "1460 2.46526e-07 [ 0.99942333] [ 0.00131089]\n",
      "1480 2.23886e-07 [ 0.99945039] [ 0.00124929]\n",
      "1500 2.03363e-07 [ 0.99947613] [ 0.00119061]\n",
      "1520 1.8475e-07 [ 0.99950075] [ 0.00113471]\n",
      "1540 1.67744e-07 [ 0.99952418] [ 0.00108142]\n",
      "1560 1.5235e-07 [ 0.99954659] [ 0.00103064]\n",
      "1580 1.38446e-07 [ 0.99956787] [ 0.00098224]\n",
      "1600 1.25736e-07 [ 0.99958819] [ 0.00093614]\n",
      "1620 1.14216e-07 [ 0.99960738] [ 0.00089221]\n",
      "1640 1.03703e-07 [ 0.9996258] [ 0.00085032]\n",
      "1660 9.41961e-08 [ 0.99964345] [ 0.00081041]\n",
      "1680 8.55878e-08 [ 0.99966019] [ 0.0007724]\n",
      "1700 7.77331e-08 [ 0.99967605] [ 0.00073613]\n",
      "1720 7.06128e-08 [ 0.99969137] [ 0.00070158]\n",
      "1740 6.41529e-08 [ 0.99970573] [ 0.00066869]\n",
      "1760 5.82659e-08 [ 0.99971962] [ 0.00063729]\n",
      "1780 5.29167e-08 [ 0.99973273] [ 0.00060742]\n",
      "1800 4.8064e-08 [ 0.99974531] [ 0.00057887]\n",
      "1820 4.36933e-08 [ 0.99975723] [ 0.00055174]\n",
      "1840 3.96913e-08 [ 0.99976861] [ 0.0005258]\n",
      "1860 3.60597e-08 [ 0.9997794] [ 0.00050119]\n",
      "1880 3.27389e-08 [ 0.99978989] [ 0.00047765]\n",
      "1900 2.97346e-08 [ 0.99979955] [ 0.00045526]\n",
      "1920 2.70208e-08 [ 0.99980909] [ 0.00043397]\n",
      "1940 2.45393e-08 [ 0.99981803] [ 0.00041356]\n",
      "1960 2.2295e-08 [ 0.99982643] [ 0.00039423]\n",
      "1980 2.02514e-08 [ 0.99983478] [ 0.00037576]\n",
      "2000 1.83809e-08 [ 0.99984246] [ 0.00035808]\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session.\n",
    "with tf.Session() as session:\n",
    "    # Initializes global variables in the graph.\n",
    "    session.run(tf.global_variables_initializer()) #Variable을 실행하기 전에 global_variables_initializer()로 초기화한다.\n",
    "    for step in range(2001):\n",
    "        session.run(train) #train을 실행시킨다.\n",
    "        if step %20 == 0: #20번 마다 상태를 출력한다.\n",
    "            print(step, session.run(cost), session.run(W), session.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train 노드 밑에는 cost가 연결되어 있고, 그 밑에는 다시 hypothesis가 연결되어 있다. 그리고 그 밑에는 W, b가 연결되는 구조. 따라서 train을 실행시키면 모든 노드와 연결되어 있다. \n",
    "<img src=\"Images/lab2_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b #provide shortcut for tf.add(a, b)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(session.run(adder_node, feed_dict={a: 3, b: 4.5}))\n",
    "    print(session.run(adder_node, feed_dict={a: [1, 3], b: [2, 4]}))\n",
    "    #tf.placeholder()는 입력 데이터를 만들 때 주로 사용한다. (실제 훈련 예제를 제공하는 변수) - 초기값을 지정할 필요 없다. (모델 입력시 변경되지 않을 데이터)\n",
    "    #tf.Variable()은 데이터의 상태를 저장할 때 주로 사용한다. (가중치나 편향 등의 학습 가능한 변수) - 초기값을 지정해야 한다. (학습 되는 데이터)\n",
    "    #http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.121176 [ 0.95037854] [-0.20876008]\n",
      "20 0.00427811 [ 1.05437529] [-0.15422663]\n",
      "40 0.00292641 [ 1.06150031] [-0.14272009]\n",
      "60 0.00264912 [ 1.05953181] [-0.13560733]\n",
      "80 0.0024059 [ 1.0568217] [-0.12919566]\n",
      "100 0.00218508 [ 1.05415988] [-0.12312035]\n",
      "120 0.00198452 [ 1.05161536] [-0.11733387]\n",
      "140 0.00180237 [ 1.04918957] [-0.11181959]\n",
      "160 0.00163695 [ 1.04687786] [-0.10656451]\n",
      "180 0.0014867 [ 1.04467487] [-0.10155638]\n",
      "200 0.00135025 [ 1.04257524] [-0.09678364]\n",
      "220 0.00122632 [ 1.04057479] [-0.09223528]\n",
      "240 0.00111377 [ 1.03866756] [-0.08790057]\n",
      "260 0.00101154 [ 1.03685033] [-0.08376953]\n",
      "280 0.000918699 [ 1.03511846] [-0.07983267]\n",
      "300 0.000834374 [ 1.03346813] [-0.07608084]\n",
      "320 0.000757795 [ 1.03189528] [-0.07250538]\n",
      "340 0.000688242 [ 1.03039634] [-0.06909797]\n",
      "360 0.000625073 [ 1.02896774] [-0.06585057]\n",
      "380 0.000567701 [ 1.02760637] [-0.0627558]\n",
      "400 0.000515594 [ 1.02630901] [-0.05980652]\n",
      "420 0.000468271 [ 1.02507269] [-0.05699587]\n",
      "440 0.000425292 [ 1.02389431] [-0.05431733]\n",
      "460 0.000386256 [ 1.02277136] [-0.05176462]\n",
      "480 0.000350805 [ 1.02170122] [-0.04933193]\n",
      "500 0.000318611 [ 1.02068174] [-0.04701366]\n",
      "520 0.000289369 [ 1.01970971] [-0.04480444]\n",
      "540 0.00026281 [ 1.01878321] [-0.04269874]\n",
      "560 0.000238687 [ 1.01790047] [-0.04069204]\n",
      "580 0.000216778 [ 1.01705921] [-0.03877964]\n",
      "600 0.000196882 [ 1.01625752] [-0.03695713]\n",
      "620 0.000178811 [ 1.01549351] [-0.03522028]\n",
      "640 0.0001624 [ 1.01476538] [-0.03356508]\n",
      "660 0.000147494 [ 1.01407146] [-0.03198763]\n",
      "680 0.000133957 [ 1.01341021] [-0.03048438]\n",
      "700 0.000121661 [ 1.01277995] [-0.02905175]\n",
      "720 0.000110496 [ 1.01217937] [-0.02768646]\n",
      "740 0.000100354 [ 1.01160693] [-0.02638531]\n",
      "760 9.11434e-05 [ 1.01106143] [-0.02514532]\n",
      "780 8.27776e-05 [ 1.01054168] [-0.02396364]\n",
      "800 7.51802e-05 [ 1.01004612] [-0.02283741]\n",
      "820 6.82792e-05 [ 1.00957406] [-0.02176403]\n",
      "840 6.20112e-05 [ 1.00912416] [-0.02074121]\n",
      "860 5.63214e-05 [ 1.00869524] [-0.01976646]\n",
      "880 5.11516e-05 [ 1.0082866] [-0.01883752]\n",
      "900 4.64569e-05 [ 1.00789714] [-0.0179522]\n",
      "920 4.21927e-05 [ 1.00752616] [-0.01710852]\n",
      "940 3.83203e-05 [ 1.00717247] [-0.01630453]\n",
      "960 3.48035e-05 [ 1.00683534] [-0.01553828]\n",
      "980 3.1608e-05 [ 1.00651395] [-0.014808]\n",
      "1000 2.87076e-05 [ 1.00620794] [-0.01411205]\n",
      "1020 2.60727e-05 [ 1.00591612] [-0.01344884]\n",
      "1040 2.36792e-05 [ 1.00563812] [-0.0128168]\n",
      "1060 2.15056e-05 [ 1.00537312] [-0.01221444]\n",
      "1080 1.95319e-05 [ 1.00512064] [-0.01164039]\n",
      "1100 1.7739e-05 [ 1.00487995] [-0.01109333]\n",
      "1120 1.61109e-05 [ 1.00465071] [-0.01057201]\n",
      "1140 1.46328e-05 [ 1.00443208] [-0.01007519]\n",
      "1160 1.32895e-05 [ 1.00422394] [-0.00960176]\n",
      "1180 1.20698e-05 [ 1.00402534] [-0.00915056]\n",
      "1200 1.09621e-05 [ 1.00383615] [-0.00872052]\n",
      "1220 9.95577e-06 [ 1.00365591] [-0.00831069]\n",
      "1240 9.04262e-06 [ 1.00348413] [-0.00792013]\n",
      "1260 8.21252e-06 [ 1.00332046] [-0.00754795]\n",
      "1280 7.45914e-06 [ 1.00316441] [-0.00719326]\n",
      "1300 6.77407e-06 [ 1.00301564] [-0.00685521]\n",
      "1320 6.15226e-06 [ 1.00287402] [-0.00653309]\n",
      "1340 5.58765e-06 [ 1.00273895] [-0.00622609]\n",
      "1360 5.07519e-06 [ 1.00261021] [-0.00593354]\n",
      "1380 4.60928e-06 [ 1.00248766] [-0.00565473]\n",
      "1400 4.18614e-06 [ 1.00237072] [-0.00538904]\n",
      "1420 3.80214e-06 [ 1.00225925] [-0.00513581]\n",
      "1440 3.45349e-06 [ 1.00215304] [-0.00489449]\n",
      "1460 3.13643e-06 [ 1.00205195] [-0.0046645]\n",
      "1480 2.84837e-06 [ 1.00195551] [-0.00444532]\n",
      "1500 2.58722e-06 [ 1.00186372] [-0.00423649]\n",
      "1520 2.3498e-06 [ 1.00177622] [-0.00403745]\n",
      "1540 2.13409e-06 [ 1.00169277] [-0.00384777]\n",
      "1560 1.93844e-06 [ 1.00161314] [-0.00366702]\n",
      "1580 1.76073e-06 [ 1.00153756] [-0.00349475]\n",
      "1600 1.59898e-06 [ 1.00146508] [-0.00333057]\n",
      "1620 1.45228e-06 [ 1.00139642] [-0.00317408]\n",
      "1640 1.31906e-06 [ 1.00133085] [-0.003025]\n",
      "1660 1.19799e-06 [ 1.00126815] [-0.00288288]\n",
      "1680 1.08812e-06 [ 1.00120866] [-0.00274746]\n",
      "1700 9.88289e-07 [ 1.00115204] [-0.00261842]\n",
      "1720 8.97704e-07 [ 1.00109792] [-0.00249547]\n",
      "1740 8.15339e-07 [ 1.00104642] [-0.00237827]\n",
      "1760 7.40573e-07 [ 1.0009973] [-0.0022666]\n",
      "1780 6.72702e-07 [ 1.00095046] [-0.00216017]\n",
      "1800 6.10935e-07 [ 1.00090575] [-0.00205873]\n",
      "1820 5.54959e-07 [ 1.00086319] [-0.00196206]\n",
      "1840 5.04088e-07 [ 1.00082254] [-0.00186995]\n",
      "1860 4.57891e-07 [ 1.00078428] [-0.00178223]\n",
      "1880 4.15909e-07 [ 1.00074756] [-0.0016986]\n",
      "1900 3.778e-07 [ 1.00071228] [-0.0016189]\n",
      "1920 3.43309e-07 [ 1.0006789] [-0.00154302]\n",
      "1940 3.11785e-07 [ 1.00064719] [-0.00147059]\n",
      "1960 2.8313e-07 [ 1.00061655] [-0.00140156]\n",
      "1980 2.57246e-07 [ 1.00058794] [-0.00133589]\n",
      "2000 2.33656e-07 [ 1.00056028] [-0.00127318]\n"
     ]
    }
   ],
   "source": [
    "# X and Y data\n",
    "# x_train = [1, 2, 3] #학습 데이터\n",
    "# y_train = [1, 2, 3] #정답 레이블\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#위에서 사용한 값을 그대로 다시 재사용하면 오류\n",
    "\n",
    "# Now we can use X and Y in place of x_data and y_data\n",
    "# # placeholders for a tensor that will be always fed using feed_dict\n",
    "# See http://stackoverflow.com/questions/36693740/\n",
    "X = tf.placeholder(tf.float32, shape=[None]) #shape 1차원이면서 요소 개수 제한 없음\n",
    "Y = tf.placeholder(tf.float32, shape=[None])\n",
    "#예전처럼 직접 입력하는 것이 아니라, Placeholder로 값을 train을 실행할 때 값을 지정해 줄 수 있다.\n",
    "\n",
    "# Our hypothesis XW+b\n",
    "hypothesis = X * W + b #가중치를 곱하고 편향을 더해 예측값을 측정한다.\n",
    "\n",
    "# cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y)) #코스트 계산\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) #최적화함수, 학습률\n",
    "train = optimizer.minimize(cost) #최적화\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer()) #초기화\n",
    "    for step in range(2001):\n",
    "        cost_val, W_val, b_val, _ = session.run([cost, W, b, train], feed_dict={X: [1, 2, 3], Y: [1, 2, 3]})\n",
    "\n",
    "        #Placeholder는 세션을 실행 시 값을 지정해 준다. - 만들어진 모델에 직접 값을 넘겨 줄 수가 있다.\n",
    "        #실행도 [cost, W, b, train]를 묶어 한 번에 할 수 있다.\n",
    "        if step %20 == 0 :\n",
    "            print(step, cost_val, W_val, b_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
