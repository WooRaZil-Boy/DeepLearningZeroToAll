{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "1 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "2 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "3 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "4 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "5 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "6 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "7 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "8 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "9 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "10 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "11 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "12 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "13 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "14 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "15 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "16 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "17 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "18 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "19 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "20 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "21 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "22 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "23 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "24 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "25 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "26 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "27 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "28 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "29 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "30 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "31 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "32 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "33 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "34 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "35 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "36 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "37 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "38 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "39 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "40 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "41 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "42 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "43 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "44 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "45 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "46 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "47 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "48 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "49 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "50 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "51 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "52 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "53 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "54 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "55 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "56 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "57 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "58 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "59 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "60 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "61 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "62 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "63 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "64 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "65 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "66 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "67 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "68 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "69 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "70 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "71 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "72 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "73 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "74 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "75 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "76 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "77 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "78 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "79 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "80 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "81 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "82 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "83 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "84 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "85 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "86 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "87 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "88 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "89 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "90 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "91 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "92 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "93 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "94 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "95 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "96 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "97 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "98 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "99 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "100 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "101 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "102 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "103 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "104 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "105 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "106 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "107 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "108 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "109 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "110 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "111 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "112 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "113 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "114 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "115 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "116 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "117 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "118 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "119 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "120 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "121 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "122 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "123 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "124 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "125 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "126 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "127 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "128 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "129 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "130 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "131 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "132 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "133 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "134 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "135 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "136 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "137 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "138 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "139 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "140 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "142 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "143 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "144 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "145 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "146 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "147 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "148 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "149 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "150 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "151 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "152 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "153 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "154 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "155 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "156 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "157 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "158 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "159 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "160 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "161 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "162 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "163 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "164 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "165 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "166 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "167 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "168 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "169 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "170 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "171 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "172 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "173 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "174 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "175 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "176 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "177 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "178 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "179 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "180 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "181 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "182 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "183 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "184 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "185 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "186 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "187 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "188 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "189 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "190 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "191 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "192 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "193 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "194 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "195 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "196 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "197 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "198 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "199 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "200 6.44227 [[ 0.85962057  1.28496659  1.46813238]\n",
      " [ 1.9539367   0.96308261  0.64203686]\n",
      " [ 1.76346159 -0.38934138  1.80743122]]\n",
      "Prediction: [2 2 0]\n",
      "Accuracy:  0.666667\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "#Training Data\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]\n",
    "#Test Data\n",
    "\n",
    "#Training 데이터와 Test 데이터를 분리해야 한다.\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "#Placeholder를 이용하기 때문에 학습 데이터와 테스트 데이터를 feed_dict 따로 넣어주기만 하면 된다.\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "#tf.placeholder()는 입력 데이터를 만들 때 주로 사용한다. (실제 훈련 예제를 제공하는 변수) - 초기값을 지정할 필요 없다. (모델 입력시 변경되지 않을 데이터)\n",
    "#tf.Variable()은 데이터의 상태를 저장할 때 주로 사용한다. (가중치나 편향 등의 학습 가능한 변수) - 초기값을 지정해야 한다. (학습 되는 데이터)\n",
    "#http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) #Cross entropy\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost) #최적화\n",
    "\n",
    "# Correct prediction Test model\n",
    "prediction = tf.arg_max(hypothesis, 1) #arg_max : 가장 큰 값의 인덱스\n",
    "is_correct = tf.equal(prediction, tf.arg_max(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer()) #초기화\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "        #Training 데이터만으로 학습을 한다.\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n",
    "    #Test 데이터만으로 예측과 정확도 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate\n",
    "<img src=\"Images/lab7_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomalize\n",
    "\n",
    "<img src=\"Images/lab7_2.png\">\n",
    "\n",
    "<img src=\"Images/lab7_3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999999  0.99999999  0.          1.          1.        ]\n",
      " [ 0.70548491  0.70439552  1.          0.71881782  0.83755791]\n",
      " [ 0.54412549  0.50274824  0.57608696  0.606468    0.6606331 ]\n",
      " [ 0.33890353  0.31368023  0.10869565  0.45989134  0.43800918]\n",
      " [ 0.51436     0.42582389  0.30434783  0.58504805  0.42624401]\n",
      " [ 0.49556179  0.42582389  0.31521739  0.48131134  0.49276137]\n",
      " [ 0.11436064  0.          0.20652174  0.22007776  0.18597238]\n",
      " [ 0.          0.07747099  0.5326087   0.          0.        ]]\n",
      "0 Cost:  1.00072 \n",
      "Prediction:\n",
      " [[-0.95883536]\n",
      " [ 0.1618742 ]\n",
      " [-0.19608256]\n",
      " [-0.50727671]\n",
      " [-0.49069303]\n",
      " [-0.44204131]\n",
      " [-0.38210535]\n",
      " [ 0.21758243]]\n",
      "1 Cost:  1.00066 \n",
      "Prediction:\n",
      " [[-0.95878714]\n",
      " [ 0.16191787]\n",
      " [-0.19604582]\n",
      " [-0.50724769]\n",
      " [-0.49065876]\n",
      " [-0.44200838]\n",
      " [-0.3820841 ]\n",
      " [ 0.21760225]]\n",
      "2 Cost:  1.0006 \n",
      "Prediction:\n",
      " [[-0.9587388 ]\n",
      " [ 0.16196147]\n",
      " [-0.19600907]\n",
      " [-0.50721884]\n",
      " [-0.49062458]\n",
      " [-0.44197544]\n",
      " [-0.38206288]\n",
      " [ 0.21762207]]\n",
      "3 Cost:  1.00053 \n",
      "Prediction:\n",
      " [[-0.95869064]\n",
      " [ 0.16200513]\n",
      " [-0.19597226]\n",
      " [-0.50718987]\n",
      " [-0.49059033]\n",
      " [-0.44194257]\n",
      " [-0.38204163]\n",
      " [ 0.21764183]]\n",
      "4 Cost:  1.00047 \n",
      "Prediction:\n",
      " [[-0.95864236]\n",
      " [ 0.16204885]\n",
      " [-0.19593552]\n",
      " [-0.5071609 ]\n",
      " [-0.49055612]\n",
      " [-0.4419097 ]\n",
      " [-0.38202038]\n",
      " [ 0.21766165]]\n",
      "5 Cost:  1.00041 \n",
      "Prediction:\n",
      " [[-0.95859402]\n",
      " [ 0.16209251]\n",
      " [-0.19589877]\n",
      " [-0.50713193]\n",
      " [-0.490522  ]\n",
      " [-0.4418768 ]\n",
      " [-0.38199914]\n",
      " [ 0.21768147]]\n",
      "6 Cost:  1.00035 \n",
      "Prediction:\n",
      " [[-0.9585458 ]\n",
      " [ 0.16213623]\n",
      " [-0.19586203]\n",
      " [-0.50710309]\n",
      " [-0.49048778]\n",
      " [-0.44184387]\n",
      " [-0.38197789]\n",
      " [ 0.21770129]]\n",
      "7 Cost:  1.00028 \n",
      "Prediction:\n",
      " [[-0.95849752]\n",
      " [ 0.16217977]\n",
      " [-0.19582522]\n",
      " [-0.50707406]\n",
      " [-0.49045354]\n",
      " [-0.441811  ]\n",
      " [-0.38195664]\n",
      " [ 0.21772105]]\n",
      "8 Cost:  1.00022 \n",
      "Prediction:\n",
      " [[-0.95844936]\n",
      " [ 0.16222349]\n",
      " [-0.19578841]\n",
      " [-0.50704515]\n",
      " [-0.49041936]\n",
      " [-0.44177809]\n",
      " [-0.38193542]\n",
      " [ 0.21774086]]\n",
      "9 Cost:  1.00016 \n",
      "Prediction:\n",
      " [[-0.95840102]\n",
      " [ 0.16226709]\n",
      " [-0.19575167]\n",
      " [-0.50701618]\n",
      " [-0.49038514]\n",
      " [-0.44174522]\n",
      " [-0.38191417]\n",
      " [ 0.21776068]]\n",
      "10 Cost:  1.00009 \n",
      "Prediction:\n",
      " [[-0.9583528 ]\n",
      " [ 0.16231081]\n",
      " [-0.19571492]\n",
      " [-0.50698721]\n",
      " [-0.4903509 ]\n",
      " [-0.44171229]\n",
      " [-0.38189295]\n",
      " [ 0.21778044]]\n",
      "11 Cost:  1.00003 \n",
      "Prediction:\n",
      " [[-0.95830452]\n",
      " [ 0.16235435]\n",
      " [-0.19567811]\n",
      " [-0.50695825]\n",
      " [-0.49031666]\n",
      " [-0.44167945]\n",
      " [-0.3818717 ]\n",
      " [ 0.21780026]]\n",
      "12 Cost:  0.999966 \n",
      "Prediction:\n",
      " [[-0.95825624]\n",
      " [ 0.16239807]\n",
      " [-0.19564143]\n",
      " [-0.50692934]\n",
      " [-0.49028251]\n",
      " [-0.44164652]\n",
      " [-0.38185045]\n",
      " [ 0.21782008]]\n",
      "13 Cost:  0.999903 \n",
      "Prediction:\n",
      " [[-0.95820802]\n",
      " [ 0.16244173]\n",
      " [-0.19560462]\n",
      " [-0.50690043]\n",
      " [-0.4902482 ]\n",
      " [-0.44161364]\n",
      " [-0.3818292 ]\n",
      " [ 0.2178399 ]]\n",
      "14 Cost:  0.99984 \n",
      "Prediction:\n",
      " [[-0.9581598 ]\n",
      " [ 0.16248533]\n",
      " [-0.19556788]\n",
      " [-0.50687152]\n",
      " [-0.49021414]\n",
      " [-0.44158086]\n",
      " [-0.38180798]\n",
      " [ 0.21785966]]\n",
      "15 Cost:  0.999777 \n",
      "Prediction:\n",
      " [[-0.95811176]\n",
      " [ 0.16252887]\n",
      " [-0.19553119]\n",
      " [-0.50684261]\n",
      " [-0.49017993]\n",
      " [-0.44154799]\n",
      " [-0.38178676]\n",
      " [ 0.21787947]]\n",
      "16 Cost:  0.999714 \n",
      "Prediction:\n",
      " [[-0.9580636 ]\n",
      " [ 0.16257247]\n",
      " [-0.1954945 ]\n",
      " [-0.50681365]\n",
      " [-0.49014583]\n",
      " [-0.44151512]\n",
      " [-0.38176551]\n",
      " [ 0.21789935]]\n",
      "17 Cost:  0.999651 \n",
      "Prediction:\n",
      " [[-0.9580155 ]\n",
      " [ 0.16261601]\n",
      " [-0.19545782]\n",
      " [-0.5067848 ]\n",
      " [-0.49011159]\n",
      " [-0.44148225]\n",
      " [-0.38174427]\n",
      " [ 0.21791911]]\n",
      "18 Cost:  0.999588 \n",
      "Prediction:\n",
      " [[-0.95796728]\n",
      " [ 0.16265962]\n",
      " [-0.19542107]\n",
      " [-0.50675589]\n",
      " [-0.49007753]\n",
      " [-0.44144952]\n",
      " [-0.38172308]\n",
      " [ 0.21793893]]\n",
      "19 Cost:  0.999525 \n",
      "Prediction:\n",
      " [[-0.95791924]\n",
      " [ 0.16270331]\n",
      " [-0.19538441]\n",
      " [-0.50672704]\n",
      " [-0.49004334]\n",
      " [-0.44141668]\n",
      " [-0.38170186]\n",
      " [ 0.21795872]]\n",
      "20 Cost:  0.999462 \n",
      "Prediction:\n",
      " [[-0.95787108]\n",
      " [ 0.16274682]\n",
      " [-0.19534776]\n",
      " [-0.50669813]\n",
      " [-0.49000934]\n",
      " [-0.4413839 ]\n",
      " [-0.38168067]\n",
      " [ 0.21797845]]\n",
      "21 Cost:  0.999399 \n",
      "Prediction:\n",
      " [[-0.95782304]\n",
      " [ 0.16279033]\n",
      " [-0.19531116]\n",
      " [-0.50666928]\n",
      " [-0.48997518]\n",
      " [-0.44135103]\n",
      " [-0.38165948]\n",
      " [ 0.21799824]]\n",
      "22 Cost:  0.999336 \n",
      "Prediction:\n",
      " [[-0.95777488]\n",
      " [ 0.16283384]\n",
      " [-0.19527438]\n",
      " [-0.50664043]\n",
      " [-0.48994109]\n",
      " [-0.4413183 ]\n",
      " [-0.38163829]\n",
      " [ 0.21801803]]\n",
      "23 Cost:  0.999273 \n",
      "Prediction:\n",
      " [[-0.95772684]\n",
      " [ 0.16287741]\n",
      " [-0.19523773]\n",
      " [-0.50661153]\n",
      " [-0.48990691]\n",
      " [-0.44128546]\n",
      " [-0.38161707]\n",
      " [ 0.21803781]]\n",
      "24 Cost:  0.99921 \n",
      "Prediction:\n",
      " [[-0.95767868]\n",
      " [ 0.16292092]\n",
      " [-0.19520107]\n",
      " [-0.50658262]\n",
      " [-0.48987287]\n",
      " [-0.44125271]\n",
      " [-0.38159588]\n",
      " [ 0.21805754]]\n",
      "25 Cost:  0.999147 \n",
      "Prediction:\n",
      " [[-0.95763063]\n",
      " [ 0.16296449]\n",
      " [-0.19516441]\n",
      " [-0.50655377]\n",
      " [-0.48983869]\n",
      " [-0.44121981]\n",
      " [-0.38157469]\n",
      " [ 0.21807733]]\n",
      "26 Cost:  0.999084 \n",
      "Prediction:\n",
      " [[-0.95758247]\n",
      " [ 0.163008  ]\n",
      " [-0.19512782]\n",
      " [-0.50652492]\n",
      " [-0.48980451]\n",
      " [-0.44118708]\n",
      " [-0.3815535 ]\n",
      " [ 0.21809712]]\n",
      "27 Cost:  0.999021 \n",
      "Prediction:\n",
      " [[-0.95753443]\n",
      " [ 0.16305158]\n",
      " [-0.1950911 ]\n",
      " [-0.50649601]\n",
      " [-0.48977047]\n",
      " [-0.44115427]\n",
      " [-0.38153228]\n",
      " [ 0.21811685]]\n",
      "28 Cost:  0.998958 \n",
      "Prediction:\n",
      " [[-0.95748627]\n",
      " [ 0.16309509]\n",
      " [-0.19505438]\n",
      " [-0.50646716]\n",
      " [-0.48973635]\n",
      " [-0.44112149]\n",
      " [-0.38151109]\n",
      " [ 0.21813664]]\n",
      "29 Cost:  0.998896 \n",
      "Prediction:\n",
      " [[-0.95743823]\n",
      " [ 0.1631386 ]\n",
      " [-0.19501778]\n",
      " [-0.50643826]\n",
      " [-0.48970225]\n",
      " [-0.44108871]\n",
      " [-0.3814899 ]\n",
      " [ 0.21815643]]\n",
      "30 Cost:  0.998833 \n",
      "Prediction:\n",
      " [[-0.95739007]\n",
      " [ 0.16318211]\n",
      " [-0.19498107]\n",
      " [-0.50640941]\n",
      " [-0.4896681 ]\n",
      " [-0.44105583]\n",
      " [-0.38146871]\n",
      " [ 0.21817622]]\n",
      "31 Cost:  0.99877 \n",
      "Prediction:\n",
      " [[-0.95734203]\n",
      " [ 0.1632258 ]\n",
      " [-0.19494435]\n",
      " [-0.50638056]\n",
      " [-0.48963404]\n",
      " [-0.44102308]\n",
      " [-0.38144749]\n",
      " [ 0.21819595]]\n",
      "32 Cost:  0.998707 \n",
      "Prediction:\n",
      " [[-0.95729387]\n",
      " [ 0.16326931]\n",
      " [-0.19490775]\n",
      " [-0.50635171]\n",
      " [-0.48959985]\n",
      " [-0.44099024]\n",
      " [-0.3814263 ]\n",
      " [ 0.21821573]]\n",
      "33 Cost:  0.998644 \n",
      "Prediction:\n",
      " [[-0.95724583]\n",
      " [ 0.16331288]\n",
      " [-0.19487104]\n",
      " [-0.5063228 ]\n",
      " [-0.48956582]\n",
      " [-0.44095755]\n",
      " [-0.38140512]\n",
      " [ 0.21823546]]\n",
      "34 Cost:  0.998581 \n",
      "Prediction:\n",
      " [[-0.95719767]\n",
      " [ 0.16335639]\n",
      " [-0.19483444]\n",
      " [-0.50629395]\n",
      " [-0.48953164]\n",
      " [-0.4409247 ]\n",
      " [-0.38138393]\n",
      " [ 0.21825525]]\n",
      "35 Cost:  0.998518 \n",
      "Prediction:\n",
      " [[-0.95714962]\n",
      " [ 0.16339996]\n",
      " [-0.19479772]\n",
      " [-0.50626504]\n",
      " [-0.48949757]\n",
      " [-0.44089186]\n",
      " [-0.38136271]\n",
      " [ 0.21827504]]\n",
      "36 Cost:  0.998455 \n",
      "Prediction:\n",
      " [[-0.95710146]\n",
      " [ 0.16344342]\n",
      " [-0.19476101]\n",
      " [-0.5062362 ]\n",
      " [-0.48946348]\n",
      " [-0.44085905]\n",
      " [-0.38134152]\n",
      " [ 0.21829483]]\n",
      "37 Cost:  0.998392 \n",
      "Prediction:\n",
      " [[-0.95705342]\n",
      " [ 0.16348699]\n",
      " [-0.19472447]\n",
      " [-0.50620735]\n",
      " [-0.48942932]\n",
      " [-0.44082633]\n",
      " [-0.38132033]\n",
      " [ 0.21831456]]\n",
      "38 Cost:  0.998329 \n",
      "Prediction:\n",
      " [[-0.95700526]\n",
      " [ 0.1635305 ]\n",
      " [-0.19468775]\n",
      " [-0.50617844]\n",
      " [-0.4893952 ]\n",
      " [-0.44079348]\n",
      " [-0.38129914]\n",
      " [ 0.21833435]]\n",
      "39 Cost:  0.998266 \n",
      "Prediction:\n",
      " [[-0.95695722]\n",
      " [ 0.16357407]\n",
      " [-0.1946511 ]\n",
      " [-0.50614959]\n",
      " [-0.48936111]\n",
      " [-0.44076067]\n",
      " [-0.38127792]\n",
      " [ 0.21835414]]\n",
      "40 Cost:  0.998203 \n",
      "Prediction:\n",
      " [[-0.95690906]\n",
      " [ 0.16361758]\n",
      " [-0.19461438]\n",
      " [-0.5061208 ]\n",
      " [-0.48932698]\n",
      " [-0.44072783]\n",
      " [-0.38125673]\n",
      " [ 0.21837386]]\n",
      "41 Cost:  0.99814 \n",
      "Prediction:\n",
      " [[-0.95686102]\n",
      " [ 0.16366109]\n",
      " [-0.19457772]\n",
      " [-0.50609183]\n",
      " [-0.48929286]\n",
      " [-0.44069508]\n",
      " [-0.38123554]\n",
      " [ 0.21839365]]\n",
      "42 Cost:  0.998078 \n",
      "Prediction:\n",
      " [[-0.95681286]\n",
      " [ 0.16370466]\n",
      " [-0.19454107]\n",
      " [-0.50606298]\n",
      " [-0.48925877]\n",
      " [-0.44066232]\n",
      " [-0.38121435]\n",
      " [ 0.21841344]]\n",
      "43 Cost:  0.998015 \n",
      "Prediction:\n",
      " [[-0.95676482]\n",
      " [ 0.16374829]\n",
      " [-0.19450441]\n",
      " [-0.50603414]\n",
      " [-0.48922464]\n",
      " [-0.44062948]\n",
      " [-0.38119316]\n",
      " [ 0.21843323]]\n",
      "44 Cost:  0.997952 \n",
      "Prediction:\n",
      " [[-0.95671678]\n",
      " [ 0.16379187]\n",
      " [-0.19446769]\n",
      " [-0.50600523]\n",
      " [-0.48919052]\n",
      " [-0.44059664]\n",
      " [-0.38117197]\n",
      " [ 0.21845296]]\n",
      "45 Cost:  0.997889 \n",
      "Prediction:\n",
      " [[-0.95666862]\n",
      " [ 0.16383532]\n",
      " [-0.19443104]\n",
      " [-0.50597644]\n",
      " [-0.48915642]\n",
      " [-0.44056389]\n",
      " [-0.38115075]\n",
      " [ 0.21847275]]\n",
      "46 Cost:  0.997826 \n",
      "Prediction:\n",
      " [[-0.95662057]\n",
      " [ 0.16387889]\n",
      " [-0.1943945 ]\n",
      " [-0.50594747]\n",
      " [-0.4891223 ]\n",
      " [-0.4405311 ]\n",
      " [-0.38112956]\n",
      " [ 0.21849254]]\n",
      "47 Cost:  0.997763 \n",
      "Prediction:\n",
      " [[-0.95657241]\n",
      " [ 0.1639224 ]\n",
      " [-0.19435772]\n",
      " [-0.50591862]\n",
      " [-0.48908818]\n",
      " [-0.44049829]\n",
      " [-0.38110837]\n",
      " [ 0.21851227]]\n",
      "48 Cost:  0.9977 \n",
      "Prediction:\n",
      " [[-0.95652437]\n",
      " [ 0.16396597]\n",
      " [-0.19432113]\n",
      " [-0.50588977]\n",
      " [-0.48905408]\n",
      " [-0.44046548]\n",
      " [-0.38108718]\n",
      " [ 0.21853206]]\n",
      "49 Cost:  0.997637 \n",
      "Prediction:\n",
      " [[-0.95647621]\n",
      " [ 0.16400942]\n",
      " [-0.19428441]\n",
      " [-0.50586092]\n",
      " [-0.48901996]\n",
      " [-0.4404327 ]\n",
      " [-0.38106599]\n",
      " [ 0.21855184]]\n",
      "50 Cost:  0.997574 \n",
      "Prediction:\n",
      " [[-0.95642817]\n",
      " [ 0.16405299]\n",
      " [-0.19424781]\n",
      " [-0.50583202]\n",
      " [-0.48898584]\n",
      " [-0.44039991]\n",
      " [-0.38104478]\n",
      " [ 0.21857163]]\n",
      "51 Cost:  0.997512 \n",
      "Prediction:\n",
      " [[-0.95638013]\n",
      " [ 0.1640965 ]\n",
      " [-0.1942111 ]\n",
      " [-0.50580317]\n",
      " [-0.48895174]\n",
      " [-0.44036713]\n",
      " [-0.38102359]\n",
      " [ 0.21859136]]\n",
      "52 Cost:  0.997449 \n",
      "Prediction:\n",
      " [[-0.95633197]\n",
      " [ 0.16414008]\n",
      " [-0.1941745 ]\n",
      " [-0.50577432]\n",
      " [-0.48891762]\n",
      " [-0.44033435]\n",
      " [-0.3810024 ]\n",
      " [ 0.21861115]]\n",
      "53 Cost:  0.997386 \n",
      "Prediction:\n",
      " [[-0.95628393]\n",
      " [ 0.16418353]\n",
      " [-0.19413772]\n",
      " [-0.50574547]\n",
      " [-0.4888835 ]\n",
      " [-0.44030151]\n",
      " [-0.38098121]\n",
      " [ 0.21863094]]\n",
      "54 Cost:  0.997323 \n",
      "Prediction:\n",
      " [[-0.95623577]\n",
      " [ 0.1642271 ]\n",
      " [-0.19410107]\n",
      " [-0.50571656]\n",
      " [-0.4888494 ]\n",
      " [-0.44026875]\n",
      " [-0.38095999]\n",
      " [ 0.21865067]]\n",
      "55 Cost:  0.99726 \n",
      "Prediction:\n",
      " [[-0.95618773]\n",
      " [ 0.16427073]\n",
      " [-0.19406441]\n",
      " [-0.50568771]\n",
      " [-0.48881528]\n",
      " [-0.44023591]\n",
      " [-0.3809388 ]\n",
      " [ 0.21867046]]\n",
      "56 Cost:  0.997197 \n",
      "Prediction:\n",
      " [[-0.95613956]\n",
      " [ 0.1643143 ]\n",
      " [-0.19402775]\n",
      " [-0.50565886]\n",
      " [-0.48878115]\n",
      " [-0.44020313]\n",
      " [-0.38091761]\n",
      " [ 0.21869025]]\n",
      "57 Cost:  0.997134 \n",
      "Prediction:\n",
      " [[-0.95609152]\n",
      " [ 0.16435775]\n",
      " [-0.19399115]\n",
      " [-0.50563002]\n",
      " [-0.48874706]\n",
      " [-0.44017035]\n",
      " [-0.38089642]\n",
      " [ 0.21871004]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 Cost:  0.997071 \n",
      "Prediction:\n",
      " [[-0.95604348]\n",
      " [ 0.16440132]\n",
      " [-0.19395438]\n",
      " [-0.50560111]\n",
      " [-0.48871297]\n",
      " [-0.44013757]\n",
      " [-0.38087523]\n",
      " [ 0.21872976]]\n",
      "59 Cost:  0.997009 \n",
      "Prediction:\n",
      " [[-0.95599532]\n",
      " [ 0.16444483]\n",
      " [-0.19391784]\n",
      " [-0.50557232]\n",
      " [-0.48867884]\n",
      " [-0.44010472]\n",
      " [-0.38085404]\n",
      " [ 0.21874955]]\n",
      "60 Cost:  0.996946 \n",
      "Prediction:\n",
      " [[-0.95594728]\n",
      " [ 0.1644884 ]\n",
      " [-0.19388112]\n",
      " [-0.50554341]\n",
      " [-0.48864481]\n",
      " [-0.440072  ]\n",
      " [-0.38083282]\n",
      " [ 0.21876934]]\n",
      "61 Cost:  0.996883 \n",
      "Prediction:\n",
      " [[-0.95589912]\n",
      " [ 0.16453186]\n",
      " [-0.19384447]\n",
      " [-0.5055145 ]\n",
      " [-0.48861063]\n",
      " [-0.44003922]\n",
      " [-0.38081166]\n",
      " [ 0.21878907]]\n",
      "62 Cost:  0.99682 \n",
      "Prediction:\n",
      " [[-0.95585108]\n",
      " [ 0.16457543]\n",
      " [-0.19380781]\n",
      " [-0.50548565]\n",
      " [-0.4885765 ]\n",
      " [-0.44000638]\n",
      " [-0.38079044]\n",
      " [ 0.21880886]]\n",
      "63 Cost:  0.996757 \n",
      "Prediction:\n",
      " [[-0.95580304]\n",
      " [ 0.16461888]\n",
      " [-0.19377121]\n",
      " [-0.50545681]\n",
      " [-0.48854241]\n",
      " [-0.43997356]\n",
      " [-0.38076925]\n",
      " [ 0.21882865]]\n",
      "64 Cost:  0.996694 \n",
      "Prediction:\n",
      " [[-0.95575494]\n",
      " [ 0.16466242]\n",
      " [-0.19373453]\n",
      " [-0.50542796]\n",
      " [-0.48850831]\n",
      " [-0.43994084]\n",
      " [-0.38074809]\n",
      " [ 0.21884835]]\n",
      "65 Cost:  0.996632 \n",
      "Prediction:\n",
      " [[-0.95570695]\n",
      " [ 0.1647059 ]\n",
      " [-0.1936979 ]\n",
      " [-0.50539911]\n",
      " [-0.48847425]\n",
      " [-0.43990809]\n",
      " [-0.38072693]\n",
      " [ 0.21886811]]\n",
      "66 Cost:  0.996569 \n",
      "Prediction:\n",
      " [[-0.95565891]\n",
      " [ 0.16474944]\n",
      " [-0.19366127]\n",
      " [-0.50537026]\n",
      " [-0.48844019]\n",
      " [-0.4398753 ]\n",
      " [-0.38070577]\n",
      " [ 0.21888787]]\n",
      "67 Cost:  0.996506 \n",
      "Prediction:\n",
      " [[-0.95561075]\n",
      " [ 0.16479304]\n",
      " [-0.19362465]\n",
      " [-0.50534141]\n",
      " [-0.48840615]\n",
      " [-0.43984252]\n",
      " [-0.38068461]\n",
      " [ 0.21890756]]\n",
      "68 Cost:  0.996443 \n",
      "Prediction:\n",
      " [[-0.95556277]\n",
      " [ 0.16483653]\n",
      " [-0.19358808]\n",
      " [-0.50531268]\n",
      " [-0.488372  ]\n",
      " [-0.43980977]\n",
      " [-0.38066342]\n",
      " [ 0.21892732]]\n",
      "69 Cost:  0.99638 \n",
      "Prediction:\n",
      " [[-0.95551467]\n",
      " [ 0.16488007]\n",
      " [-0.19355133]\n",
      " [-0.50528377]\n",
      " [-0.48833799]\n",
      " [-0.43977705]\n",
      " [-0.38064229]\n",
      " [ 0.21894708]]\n",
      "70 Cost:  0.996318 \n",
      "Prediction:\n",
      " [[-0.95546663]\n",
      " [ 0.16492349]\n",
      " [-0.19351476]\n",
      " [-0.50525498]\n",
      " [-0.48830384]\n",
      " [-0.43974426]\n",
      " [-0.38062111]\n",
      " [ 0.21896684]]\n",
      "71 Cost:  0.996255 \n",
      "Prediction:\n",
      " [[-0.95541859]\n",
      " [ 0.16496703]\n",
      " [-0.19347808]\n",
      " [-0.50522614]\n",
      " [-0.48826984]\n",
      " [-0.43971145]\n",
      " [-0.38059998]\n",
      " [ 0.21898654]]\n",
      "72 Cost:  0.996192 \n",
      "Prediction:\n",
      " [[-0.95537049]\n",
      " [ 0.16501051]\n",
      " [-0.19344151]\n",
      " [-0.50519729]\n",
      " [-0.48823571]\n",
      " [-0.43967873]\n",
      " [-0.38057879]\n",
      " [ 0.2190063 ]]\n",
      "73 Cost:  0.996129 \n",
      "Prediction:\n",
      " [[-0.9553225 ]\n",
      " [ 0.16505399]\n",
      " [-0.19340482]\n",
      " [-0.50516844]\n",
      " [-0.48820168]\n",
      " [-0.43964598]\n",
      " [-0.38055763]\n",
      " [ 0.21902606]]\n",
      "74 Cost:  0.996067 \n",
      "Prediction:\n",
      " [[-0.95527446]\n",
      " [ 0.16509748]\n",
      " [-0.19336826]\n",
      " [-0.50513959]\n",
      " [-0.48816758]\n",
      " [-0.43961319]\n",
      " [-0.38053647]\n",
      " [ 0.21904576]]\n",
      "75 Cost:  0.996004 \n",
      "Prediction:\n",
      " [[-0.9552263 ]\n",
      " [ 0.16514102]\n",
      " [-0.19333151]\n",
      " [-0.50511074]\n",
      " [-0.48813358]\n",
      " [-0.43958047]\n",
      " [-0.38051531]\n",
      " [ 0.21906552]]\n",
      "76 Cost:  0.995941 \n",
      "Prediction:\n",
      " [[-0.95517832]\n",
      " [ 0.16518444]\n",
      " [-0.193295  ]\n",
      " [-0.50508201]\n",
      " [-0.48809943]\n",
      " [-0.43954763]\n",
      " [-0.38049412]\n",
      " [ 0.21908528]]\n",
      "77 Cost:  0.995878 \n",
      "Prediction:\n",
      " [[-0.95513034]\n",
      " [ 0.16522798]\n",
      " [-0.19325843]\n",
      " [-0.5050531 ]\n",
      " [-0.48806542]\n",
      " [-0.43951494]\n",
      " [-0.38047299]\n",
      " [ 0.21910504]]\n",
      "78 Cost:  0.995816 \n",
      "Prediction:\n",
      " [[-0.95508218]\n",
      " [ 0.1652714 ]\n",
      " [-0.19322169]\n",
      " [-0.50502431]\n",
      " [-0.4880313 ]\n",
      " [-0.43948215]\n",
      " [-0.3804518 ]\n",
      " [ 0.21912473]]\n",
      "79 Cost:  0.995753 \n",
      "Prediction:\n",
      " [[-0.95503414]\n",
      " [ 0.16531506]\n",
      " [-0.19318512]\n",
      " [-0.50499547]\n",
      " [-0.48799726]\n",
      " [-0.43944943]\n",
      " [-0.38043067]\n",
      " [ 0.21914449]]\n",
      "80 Cost:  0.99569 \n",
      "Prediction:\n",
      " [[-0.95498616]\n",
      " [ 0.16535854]\n",
      " [-0.19314849]\n",
      " [-0.50496662]\n",
      " [-0.48796314]\n",
      " [-0.43941656]\n",
      " [-0.38040948]\n",
      " [ 0.21916425]]\n",
      "81 Cost:  0.995627 \n",
      "Prediction:\n",
      " [[-0.95493805]\n",
      " [ 0.16540202]\n",
      " [-0.19311187]\n",
      " [-0.50493777]\n",
      " [-0.48792902]\n",
      " [-0.43938392]\n",
      " [-0.38038832]\n",
      " [ 0.21918395]]\n",
      "82 Cost:  0.995565 \n",
      "Prediction:\n",
      " [[-0.95489001]\n",
      " [ 0.16544551]\n",
      " [-0.19307524]\n",
      " [-0.50490892]\n",
      " [-0.48789504]\n",
      " [-0.43935111]\n",
      " [-0.38036716]\n",
      " [ 0.21920371]]\n",
      "83 Cost:  0.995502 \n",
      "Prediction:\n",
      " [[-0.95484197]\n",
      " [ 0.16548899]\n",
      " [-0.19303855]\n",
      " [-0.50488007]\n",
      " [-0.48786089]\n",
      " [-0.43931839]\n",
      " [-0.380346  ]\n",
      " [ 0.21922347]]\n",
      "84 Cost:  0.995439 \n",
      "Prediction:\n",
      " [[-0.95479387]\n",
      " [ 0.16553247]\n",
      " [-0.19300199]\n",
      " [-0.50485128]\n",
      " [-0.48782691]\n",
      " [-0.43928567]\n",
      " [-0.38032484]\n",
      " [ 0.21924323]]\n",
      "85 Cost:  0.995376 \n",
      "Prediction:\n",
      " [[-0.95474589]\n",
      " [ 0.16557595]\n",
      " [-0.19296536]\n",
      " [-0.50482249]\n",
      " [-0.48779276]\n",
      " [-0.43925285]\n",
      " [-0.38030368]\n",
      " [ 0.21926293]]\n",
      "86 Cost:  0.995314 \n",
      "Prediction:\n",
      " [[-0.95469785]\n",
      " [ 0.16561943]\n",
      " [-0.19292867]\n",
      " [-0.50479364]\n",
      " [-0.48775873]\n",
      " [-0.43922007]\n",
      " [-0.38028249]\n",
      " [ 0.21928269]]\n",
      "87 Cost:  0.995251 \n",
      "Prediction:\n",
      " [[-0.95464969]\n",
      " [ 0.16566297]\n",
      " [-0.1928921 ]\n",
      " [-0.5047648 ]\n",
      " [-0.48772463]\n",
      " [-0.43918729]\n",
      " [-0.38026136]\n",
      " [ 0.21930245]]\n",
      "88 Cost:  0.995188 \n",
      "Prediction:\n",
      " [[-0.95460171]\n",
      " [ 0.1657064 ]\n",
      " [-0.19285548]\n",
      " [-0.50473595]\n",
      " [-0.4876906 ]\n",
      " [-0.43915462]\n",
      " [-0.38024017]\n",
      " [ 0.21932214]]\n",
      "89 Cost:  0.995126 \n",
      "Prediction:\n",
      " [[-0.95455372]\n",
      " [ 0.16574994]\n",
      " [-0.19281885]\n",
      " [-0.5047071 ]\n",
      " [-0.4876565 ]\n",
      " [-0.43912175]\n",
      " [-0.38021904]\n",
      " [ 0.2193419 ]]\n",
      "90 Cost:  0.995063 \n",
      "Prediction:\n",
      " [[-0.95450568]\n",
      " [ 0.16579342]\n",
      " [-0.19278222]\n",
      " [-0.50467831]\n",
      " [-0.48762244]\n",
      " [-0.43908906]\n",
      " [-0.38019785]\n",
      " [ 0.21936166]]\n",
      "91 Cost:  0.995 \n",
      "Prediction:\n",
      " [[-0.95445752]\n",
      " [ 0.16583702]\n",
      " [-0.1927456 ]\n",
      " [-0.50464946]\n",
      " [-0.48758838]\n",
      " [-0.43905625]\n",
      " [-0.38017669]\n",
      " [ 0.21938142]]\n",
      "92 Cost:  0.994937 \n",
      "Prediction:\n",
      " [[-0.95440954]\n",
      " [ 0.16588056]\n",
      " [-0.19270897]\n",
      " [-0.50462061]\n",
      " [-0.48755428]\n",
      " [-0.43902355]\n",
      " [-0.38015553]\n",
      " [ 0.21940112]]\n",
      "93 Cost:  0.994875 \n",
      "Prediction:\n",
      " [[-0.95436156]\n",
      " [ 0.16592398]\n",
      " [-0.19267234]\n",
      " [-0.50459182]\n",
      " [-0.48752022]\n",
      " [-0.4389908 ]\n",
      " [-0.38013437]\n",
      " [ 0.21942088]]\n",
      "94 Cost:  0.994812 \n",
      "Prediction:\n",
      " [[-0.9543134 ]\n",
      " [ 0.16596752]\n",
      " [-0.19263577]\n",
      " [-0.50456297]\n",
      " [-0.48748612]\n",
      " [-0.43895799]\n",
      " [-0.38011321]\n",
      " [ 0.21944064]]\n",
      "95 Cost:  0.994749 \n",
      "Prediction:\n",
      " [[-0.95426536]\n",
      " [ 0.16601095]\n",
      " [-0.19259909]\n",
      " [-0.50453413]\n",
      " [-0.48745206]\n",
      " [-0.43892521]\n",
      " [-0.38009205]\n",
      " [ 0.21946034]]\n",
      "96 Cost:  0.994687 \n",
      "Prediction:\n",
      " [[-0.95421737]\n",
      " [ 0.16605449]\n",
      " [-0.19256246]\n",
      " [-0.50450534]\n",
      " [-0.487418  ]\n",
      " [-0.43889254]\n",
      " [-0.38007089]\n",
      " [ 0.2194801 ]]\n",
      "97 Cost:  0.994624 \n",
      "Prediction:\n",
      " [[-0.95416939]\n",
      " [ 0.16609791]\n",
      " [-0.19252589]\n",
      " [-0.50447649]\n",
      " [-0.48738393]\n",
      " [-0.43885976]\n",
      " [-0.38004974]\n",
      " [ 0.21949986]]\n",
      "98 Cost:  0.994561 \n",
      "Prediction:\n",
      " [[-0.95412123]\n",
      " [ 0.16614145]\n",
      " [-0.19248927]\n",
      " [-0.50444764]\n",
      " [-0.48734984]\n",
      " [-0.43882695]\n",
      " [-0.38002858]\n",
      " [ 0.21951956]]\n",
      "99 Cost:  0.994498 \n",
      "Prediction:\n",
      " [[-0.95407319]\n",
      " [ 0.16618487]\n",
      " [-0.19245264]\n",
      " [-0.50441885]\n",
      " [-0.4873158 ]\n",
      " [-0.4387942 ]\n",
      " [-0.38000742]\n",
      " [ 0.21953931]]\n",
      "100 Cost:  0.994436 \n",
      "Prediction:\n",
      " [[-0.95402521]\n",
      " [ 0.16622841]\n",
      " [-0.19241595]\n",
      " [-0.50439   ]\n",
      " [-0.48728171]\n",
      " [-0.43876147]\n",
      " [-0.37998626]\n",
      " [ 0.21955907]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0) #데이터 - 최소값. np.min(data, 0) : data의 최소값, 0차원에 대해. 열에 대한 최소값이 구해진다.\n",
    "    denominator = np.max(data, 0) - np.min(data, 0) #최대값 - 최소값\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7) #0으로 나눠지는 것을 막기 위해 작은 값을 더한다.\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "#데이터가 크거나 들쑥날쑥한 경우에는 정규화를 해줘야 한다.\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = MinMaxScaler(xy)\n",
    "print(xy)\n",
    "#정규화하지 않고 진행할 경우 값이 발산해서 Nan이 나온다.\n",
    "\n",
    "x_data = xy[:, 0:-1] #전체 행을 가져온다, 각 행의 0 ~ index-1 행까지 가져온다.\n",
    "y_data = xy[:, [-1]] #전체 행을 가져온다, 각 행의 마지막 행만 가져온다.\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4]) #전체 요소의 개수는 n, 각 요소는 4개씩\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1]) #전체 요소의 개수는 n, 각 요소는 1개씩\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "#tf.placeholder()는 입력 데이터를 만들 때 주로 사용한다. (실제 훈련 예제를 제공하는 변수) - 초기값을 지정할 필요 없다. (모델 입력시 변경되지 않을 데이터)\n",
    "#tf.Variable()은 데이터의 상태를 저장할 때 주로 사용한다. (가중치나 편향 등의 학습 가능한 변수) - 초기값을 지정해야 한다. (학습 되는 데이터)\n",
    "#http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y)) #평균 제곱 오차\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
    "train = optimizer.minimize(cost) #최적화\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer()) #초기화\n",
    "    \n",
    "    for step in range(101):\n",
    "        cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 2.609114408\n",
      "Epoch: 0002 cost = 1.065122392\n",
      "Epoch: 0003 cost = 0.856706764\n",
      "Epoch: 0004 cost = 0.753912530\n",
      "Epoch: 0005 cost = 0.689749216\n",
      "Epoch: 0006 cost = 0.643747267\n",
      "Epoch: 0007 cost = 0.608576023\n",
      "Epoch: 0008 cost = 0.581048547\n",
      "Epoch: 0009 cost = 0.558474733\n",
      "Epoch: 0010 cost = 0.539308574\n",
      "Epoch: 0011 cost = 0.522599912\n",
      "Epoch: 0012 cost = 0.508082677\n",
      "Epoch: 0013 cost = 0.495561296\n",
      "Epoch: 0014 cost = 0.483565744\n",
      "Epoch: 0015 cost = 0.473545129\n",
      "Learning finished\n",
      "Accuracy:  0.8905\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784]) #인풋. 28 x 28 형 이미지라 784\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes]) #정답 레이블. 10개. one-hot encoding\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes])) #가중치의 shape는 들어오는 값 input(784), 나가는 값 output(nb_classes)\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))#편향은 출력값과 같다.\n",
    "#레이어의 개수에 따라 shape가 달라진다.\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b) #활성 함수\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) #Cross Entropy\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) #최적화\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1)) #축을 기준으로 예측값과 정답레이블의 정답 인덱스(값이 가장 큰 요소) 비교\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) #정확도 계산\n",
    "\n",
    "# parameters\n",
    "training_epochs = 15 #전체 데이터 세트를 한 번 다 학습 시키는 것이 1 epoch\n",
    "batch_size = 100 #한 번에 100개씩 학습(epoch이 더 큰 개념. 한 에폭 안에 100개씩 잘라서 학습)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer()) #초기화\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs): #15번 에폭 반복\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size) #배치 사이즈에 따라 몇 번의 반복을 해야 될지 계산\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size) #next_batch를 통해 다음 100개씩 잘라서 가져온다.\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys}) #가져온 데이터를 넣고 session.run()\n",
    "            avg_cost += c / total_batch #?? total_batch가 맞나? batch_size아님?\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    #session.run() 대신에 accuracy.eval을 사용해도 된다.\n",
    "    #인풋 : 이미지, 정답레이블\n",
    "    #학습할 때 사용한 데이터 대신 테스트 데이터를 사용해야 한다.\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1) #랜덤하게 하나의 값을 불러 온다.\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    # don't know why this makes Travis Build error.\n",
    "    # plt.imshow(\n",
    "    #     mnist.test.images[r:r + 1].reshape(28, 28),\n",
    "    #     cmap='Greys',\n",
    "    #     interpolation='nearest')\n",
    "    # plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
